{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f0ca8ba-d913-45a6-a1ca-82a6bff8d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, Tuple, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "import albumentations\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\n",
    "\n",
    "import timm\n",
    "\n",
    "from warmup_scheduler import GradualWarmupScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf69705b-c586-4808-a152-f94863ca5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a8a088-bb55-4f8c-83c9-68dcd8607fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "model = nn.Linear(10, 20)\n",
    "optimizer0 = torch.optim.Adam(model.parameters(), lr=1)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer0, n_epochs-1)\n",
    "scheduler0 = GradualWarmupSchedulerV2(optimizer0, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "optimizer1 = torch.optim.Adam(model.parameters(), lr=1)\n",
    "scheduler1 = OneCycleLR(optimizer1, max_lr=1, steps_per_epoch=10, pct_start=0.1, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a418035f-07d8-4888-a104-42d6dba264b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "========\n",
      "========\n",
      "--------\n",
      "-8.0000000\n",
      "0.0689475\n",
      "--------\n",
      "-8.0000000\n",
      "0.1522987\n",
      "--------\n",
      "-8.0000000\n",
      "0.2800000\n",
      "--------\n",
      "-8.0000000\n",
      "0.4366489\n",
      "--------\n",
      "-8.0000000\n",
      "0.6033511\n",
      "--------\n",
      "-8.0000000\n",
      "0.7600000\n",
      "--------\n",
      "-8.0000000\n",
      "0.8877013\n",
      "--------\n",
      "-8.0000000\n",
      "0.9710525\n",
      "--------\n",
      "-8.0000000\n",
      "1.0000000\n",
      "--------\n",
      "-8.0000000\n",
      "0.9996954\n",
      "========\n",
      "========\n",
      "========\n",
      "--------\n",
      "1.0000000\n",
      "0.9987820\n",
      "--------\n",
      "1.0000000\n",
      "0.9972610\n",
      "--------\n",
      "1.0000000\n",
      "0.9951341\n",
      "--------\n",
      "1.0000000\n",
      "0.9924039\n",
      "--------\n",
      "1.0000000\n",
      "0.9890738\n",
      "--------\n",
      "1.0000000\n",
      "0.9851479\n",
      "--------\n",
      "1.0000000\n",
      "0.9806309\n",
      "--------\n",
      "1.0000000\n",
      "0.9755284\n",
      "--------\n",
      "1.0000000\n",
      "0.9698464\n",
      "--------\n",
      "1.0000000\n",
      "0.9635921\n",
      "========\n",
      "========\n",
      "========\n",
      "--------\n",
      "10.0000000\n",
      "0.9567729\n",
      "--------\n",
      "10.0000000\n",
      "0.9493972\n",
      "--------\n",
      "10.0000000\n",
      "0.9414740\n",
      "--------\n",
      "10.0000000\n",
      "0.9330130\n",
      "--------\n",
      "10.0000000\n",
      "0.9240244\n",
      "--------\n",
      "10.0000000\n",
      "0.9145191\n",
      "--------\n",
      "10.0000000\n",
      "0.9045089\n",
      "--------\n",
      "10.0000000\n",
      "0.8940058\n",
      "--------\n",
      "10.0000000\n",
      "0.8830227\n",
      "--------\n",
      "10.0000000\n",
      "0.8715729\n",
      "========\n",
      "========\n",
      "========\n",
      "--------\n",
      "10.0000000\n",
      "0.8596705\n",
      "--------\n",
      "10.0000000\n",
      "0.8473298\n",
      "--------\n",
      "10.0000000\n",
      "0.8345660\n",
      "--------\n",
      "10.0000000\n",
      "0.8213945\n",
      "--------\n",
      "10.0000000\n",
      "0.8078315\n",
      "--------\n",
      "10.0000000\n",
      "0.7938935\n",
      "--------\n",
      "10.0000000\n",
      "0.7795973\n",
      "--------\n",
      "10.0000000\n",
      "0.7649606\n",
      "--------\n",
      "10.0000000\n",
      "0.7500010\n",
      "--------\n",
      "10.0000000\n",
      "0.7347368\n",
      "========\n",
      "========\n",
      "========\n",
      "--------\n",
      "8.8302222\n",
      "0.7191867\n",
      "--------\n",
      "8.8302222\n",
      "0.7033695\n",
      "--------\n",
      "8.8302222\n",
      "0.6873045\n",
      "--------\n",
      "8.8302222\n",
      "0.6710114\n",
      "--------\n",
      "8.8302222\n",
      "0.6545099\n",
      "--------\n",
      "8.8302222\n",
      "0.6378201\n",
      "--------\n",
      "8.8302222\n",
      "0.6209625\n",
      "--------\n",
      "8.8302222\n",
      "0.6039574\n",
      "--------\n",
      "8.8302222\n",
      "0.5868257\n",
      "--------\n",
      "8.8302222\n",
      "0.5695883\n",
      "========\n",
      "========\n",
      "========\n",
      "--------\n",
      "7.5000000\n",
      "0.5522660\n",
      "--------\n",
      "7.5000000\n",
      "0.5348801\n",
      "--------\n",
      "7.5000000\n",
      "0.5174517\n",
      "--------\n",
      "7.5000000\n",
      "0.5000020\n",
      "--------\n",
      "7.5000000\n",
      "0.4825523\n",
      "--------\n",
      "7.5000000\n",
      "0.4651239\n",
      "--------\n",
      "7.5000000\n",
      "0.4477380\n",
      "--------\n",
      "7.5000000\n",
      "0.4304157\n",
      "--------\n",
      "7.5000000\n",
      "0.4131783\n",
      "--------\n",
      "7.5000000\n",
      "0.3960466\n",
      "========\n",
      "========\n",
      "========\n",
      "--------\n",
      "5.8682409\n",
      "0.3790415\n",
      "--------\n",
      "5.8682409\n",
      "0.3621839\n",
      "--------\n",
      "5.8682409\n",
      "0.3454941\n",
      "--------\n",
      "5.8682409\n",
      "0.3289926\n",
      "--------\n",
      "5.8682409\n",
      "0.3126995\n",
      "--------\n",
      "5.8682409\n",
      "0.2966345\n",
      "--------\n",
      "5.8682409\n",
      "0.2808173\n",
      "--------\n",
      "5.8682409\n",
      "0.2652672\n",
      "--------\n",
      "5.8682409\n",
      "0.2500030\n",
      "--------\n",
      "5.8682409\n",
      "0.2350434\n",
      "========\n",
      "========\n",
      "========\n",
      "--------\n",
      "4.1317591\n",
      "0.2204067\n",
      "--------\n",
      "4.1317591\n",
      "0.2061105\n",
      "--------\n",
      "4.1317591\n",
      "0.1921725\n",
      "--------\n",
      "4.1317591\n",
      "0.1786095\n",
      "--------\n",
      "4.1317591\n",
      "0.1654380\n",
      "--------\n",
      "4.1317591\n",
      "0.1526742\n",
      "--------\n",
      "4.1317591\n",
      "0.1403335\n",
      "--------\n",
      "4.1317591\n",
      "0.1284311\n",
      "--------\n",
      "4.1317591\n",
      "0.1169813\n",
      "--------\n",
      "4.1317591\n",
      "0.1059982\n",
      "========\n",
      "========\n",
      "========\n",
      "--------\n",
      "2.5000000\n",
      "0.0954951\n",
      "--------\n",
      "2.5000000\n",
      "0.0854849\n",
      "--------\n",
      "2.5000000\n",
      "0.0759796\n",
      "--------\n",
      "2.5000000\n",
      "0.0669910\n",
      "--------\n",
      "2.5000000\n",
      "0.0585300\n",
      "--------\n",
      "2.5000000\n",
      "0.0506068\n",
      "--------\n",
      "2.5000000\n",
      "0.0432311\n",
      "--------\n",
      "2.5000000\n",
      "0.0364119\n",
      "--------\n",
      "2.5000000\n",
      "0.0301576\n",
      "--------\n",
      "2.5000000\n",
      "0.0244756\n",
      "========\n",
      "========\n",
      "========\n",
      "--------\n",
      "1.1697778\n",
      "0.0193731\n",
      "--------\n",
      "1.1697778\n",
      "0.0148561\n",
      "--------\n",
      "1.1697778\n",
      "0.0109302\n",
      "--------\n",
      "1.1697778\n",
      "0.0076001\n",
      "--------\n",
      "1.1697778\n",
      "0.0048699\n",
      "--------\n",
      "1.1697778\n",
      "0.0027430\n",
      "--------\n",
      "1.1697778\n",
      "0.0012220\n",
      "--------\n",
      "1.1697778\n",
      "0.0003086\n",
      "--------\n",
      "1.1697778\n",
      "0.0000040\n",
      "--------\n",
      "1.1697778\n",
      "0.0003086\n",
      "========\n",
      "========\n",
      "========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan/miniconda3/envs/FULL_ML/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:971: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tried to step 102 times. The specified number of total steps is 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a8dcf8388f04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscheduler0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mscheduler1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:.7f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/FULL_ML/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH_DEPRECATION_WARNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/FULL_ML/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mget_lr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep_num\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m             raise ValueError(\"Tried to step {} times. The specified number of total steps is {}\"\n\u001b[0m\u001b[1;32m   1297\u001b[0m                              .format(step_num + 1, self.total_steps))\n\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tried to step 102 times. The specified number of total steps is 100"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs+1):\n",
    "    print('========')\n",
    "    print('========')\n",
    "    print('========')\n",
    "    scheduler0.step(epoch - 1)\n",
    "    for _ in range(10):\n",
    "        scheduler1.step()\n",
    "        print('--------')\n",
    "        print('{:.7f}'.format(optimizer0.param_groups[0][\"lr\"]))\n",
    "        print('{:.7f}'.format(optimizer1.param_groups[0][\"lr\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30490979-648e-4768-b6bf-76784472ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# utils\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d592ec-3a42-49c0-acd4-7b0801807aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# data pipeline definitions\n",
    "\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, csv, mode, transform=None):\n",
    "\n",
    "        self.csv = csv.reset_index()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.csv.iloc[index]\n",
    "\n",
    "        image = cv2.imread(row.filepath)[:,:,::-1]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            res = self.transform(image=image)\n",
    "            image = res['image'].astype(np.float32)\n",
    "        else:\n",
    "            image = image.astype(np.float32)\n",
    "\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        if self.mode == 'test':\n",
    "            return torch.tensor(image)\n",
    "        else:\n",
    "            return torch.tensor(image), torch.tensor(row.landmark_id)\n",
    "\n",
    "\n",
    "def get_transforms():\n",
    "\n",
    "    transforms_train = albumentations.Compose([\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.ImageCompression(quality_lower=99, quality_upper=100),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n",
    "        albumentations.Cutout(max_h_size=int(IMAGE_SIZE * 0.4), max_w_size=int(IMAGE_SIZE * 0.4), num_holes=1, p=0.5),\n",
    "        albumentations.Normalize()\n",
    "    ])\n",
    "\n",
    "    transforms_val = albumentations.Compose([\n",
    "        albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Normalize()\n",
    "    ])\n",
    "\n",
    "    return transforms_train, transforms_val\n",
    "\n",
    "\n",
    "def get_df():\n",
    "\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, 'train_0.csv'))\n",
    "\n",
    "    if TRAIN_STEP == 0:\n",
    "        # df_train = pd.read_csv(os.path.join(DATA_DIR, 'train_url.csv')).drop(columns=['url'])\n",
    "        df_train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "    else:\n",
    "        cls_81313 = df.landmark_id.unique()\n",
    "        # df_train = pd.read_csv(os.path.join(DATA_DIR, 'train_url.csv')).drop(columns=['url']).set_index('landmark_id').loc[cls_81313].reset_index()\n",
    "        df_train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv')).set_index('landmark_id').loc[cls_81313].reset_index()\n",
    "        \n",
    "    df_train['filepath'] = df_train['id'].apply(lambda x: os.path.join(DATA_DIR, 'train', x[0], x[1], x[2], f'{x}.jpg'))\n",
    "    df = df_train.merge(df, on=['id','landmark_id'], how='left')\n",
    "\n",
    "    landmark_id2idx = {landmark_id: idx for idx, landmark_id in enumerate(sorted(df['landmark_id'].unique()))}\n",
    "    idx2landmark_id = {idx: landmark_id for idx, landmark_id in enumerate(sorted(df['landmark_id'].unique()))}\n",
    "    df['landmark_id'] = df['landmark_id'].map(landmark_id2idx)\n",
    "\n",
    "    out_dim = df.landmark_id.nunique()\n",
    "\n",
    "    return df, out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b38e724-5c19-45c8-ae5a-65875a9bf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# model definitions\n",
    "\n",
    "class Swish(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = torch.sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "\n",
    "class Swish_module(nn.Module):\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        return Swish.apply(x)\n",
    "\n",
    "\n",
    "class DenseCrossEntropy(nn.Module):\n",
    "    @autocast()\n",
    "    def forward(self, x, target):\n",
    "        x = x.float()\n",
    "        target = target.float()\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "\n",
    "        loss = -logprobs * target\n",
    "        loss = loss.sum(-1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class ArcMarginProduct_subcenter(nn.Module):\n",
    "    def __init__(self, in_features, out_features, k=3):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n",
    "        self.reset_parameters()\n",
    "        self.k = k\n",
    "        self.out_features = out_features\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    @autocast()\n",
    "    def forward(self, features):\n",
    "        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n",
    "        cosine, _ = torch.max(cosine_all, dim=2)\n",
    "        return cosine   \n",
    "\n",
    "\n",
    "class ArcFaceLossAdaptiveMargin(nn.modules.Module):\n",
    "    def __init__(self, margins, s=30.0):\n",
    "        super().__init__()\n",
    "        self.crit = DenseCrossEntropy()\n",
    "        self.s = s\n",
    "        self.margins = margins\n",
    "            \n",
    "    @autocast()\n",
    "    def forward(self, logits, labels, out_dim):\n",
    "        ms = []\n",
    "        ms = self.margins[labels.cpu().numpy()]\n",
    "        cos_m = torch.from_numpy(np.cos(ms)).float().cuda()\n",
    "        sin_m = torch.from_numpy(np.sin(ms)).float().cuda()\n",
    "        th = torch.from_numpy(np.cos(math.pi - ms)).float().cuda()\n",
    "        mm = torch.from_numpy(np.sin(math.pi - ms) * ms).float().cuda()\n",
    "        labels = F.one_hot(labels, out_dim).float()\n",
    "        logits = logits.float()\n",
    "        cosine = logits\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * cos_m.view(-1, 1) - sine * sin_m.view(-1, 1)\n",
    "        phi = torch.where(cosine > th.view(-1, 1), phi, cosine - mm.view(-1, 1))\n",
    "        output = (labels * phi) + ((1.0 - labels) * cosine)\n",
    "        output *= self.s\n",
    "        loss = self.crit(output, labels)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.p=p\n",
    "        self.eps=eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x.clamp(min=self.eps).pow(self.p), (x.size(-2), x.size(-1))).pow(1./self.p)\n",
    "\n",
    "\n",
    "class RexNet20_Landmark(nn.Module):\n",
    "\n",
    "    def __init__(self, out_dim, load_pretrained=True):\n",
    "        super(RexNet20_Landmark, self).__init__()\n",
    "\n",
    "        self.net = timm.create_model('rexnet_200', pretrained=load_pretrained)\n",
    "        self.feat = nn.Linear(self.net.features[-1].out_channels, 512)\n",
    "        self.swish = Swish_module()\n",
    "        self.metric_classify = ArcMarginProduct_subcenter(512, out_dim)\n",
    "        self.net.head.global_pool = GeM()\n",
    "        self.net.head.fc = nn.Identity()\n",
    "\n",
    "    def extract(self, x):\n",
    "        return self.net(x).squeeze()\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.extract(x)\n",
    "        logits_m = self.metric_classify(self.swish(self.feat(x)))\n",
    "        return logits_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39590533-1117-48d0-a5ed-a1b7437c33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# training utils\n",
    "\n",
    "def global_average_precision_score(\n",
    "        y_true: Dict[Any, Any],\n",
    "        y_pred: Dict[Any, Tuple[Any, float]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute Global Average Precision score (GAP)\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : Dict[Any, Any]\n",
    "        Dictionary with query ids and true ids for query samples\n",
    "    y_pred : Dict[Any, Tuple[Any, float]]\n",
    "        Dictionary with query ids and predictions (predicted id, confidence\n",
    "        level)\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        GAP score\n",
    "    \"\"\"\n",
    "    indexes = list(y_pred.keys())\n",
    "    indexes.sort(\n",
    "        key=lambda x: -y_pred[x][1],\n",
    "    )\n",
    "    queries_with_target = len([i for i in y_true.values() if i is not None])\n",
    "    correct_predictions = 0\n",
    "    total_score = 0.\n",
    "    for i, k in enumerate(indexes, 1):\n",
    "        relevance_of_prediction_i = 0\n",
    "        if y_true[k] == y_pred[k][0]:\n",
    "            correct_predictions += 1\n",
    "            relevance_of_prediction_i = 1\n",
    "        precision_at_rank_i = correct_predictions / i\n",
    "        total_score += precision_at_rank_i * relevance_of_prediction_i\n",
    "\n",
    "    return 1 / queries_with_target * total_score\n",
    "    \n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, scaler):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for (data, target) in bar:\n",
    "\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if not USE_AMP:\n",
    "            logits_m = model(data)\n",
    "            loss = criterion(logits_m, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with autocast():\n",
    "                logits_m = model(data)\n",
    "                loss = criterion(logits_m, target)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "            \n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "        train_loss.append(loss_np)\n",
    "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
    "        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def val_epoch(model, valid_loader, criterion, get_output=False):\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    PRODS_M = []\n",
    "    PREDS_M = []\n",
    "    TARGETS = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in tqdm(valid_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            logits_m = model(data)\n",
    "\n",
    "            lmax_m = logits_m.max(1)\n",
    "            probs_m = lmax_m.values\n",
    "            preds_m = lmax_m.indices\n",
    "\n",
    "            PRODS_M.append(probs_m.detach().cpu())\n",
    "            PREDS_M.append(preds_m.detach().cpu())\n",
    "            TARGETS.append(target.detach().cpu())\n",
    "\n",
    "            loss = criterion(logits_m, target)\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        val_loss = np.mean(val_loss)\n",
    "        PRODS_M = torch.cat(PRODS_M).numpy()\n",
    "        PREDS_M = torch.cat(PREDS_M).numpy()\n",
    "        TARGETS = torch.cat(TARGETS)\n",
    "\n",
    "    if get_output:\n",
    "        return LOGITS_M\n",
    "    else:\n",
    "        acc_m = (PREDS_M == TARGETS.numpy()).mean() * 100.\n",
    "        y_true = {idx: target if target >=0 else None for idx, target in enumerate(TARGETS)}\n",
    "        y_pred_m = {idx: (pred_cls, conf) for idx, (pred_cls, conf) in enumerate(zip(PREDS_M, PRODS_M))}\n",
    "        gap_m = global_average_precision_score(y_true, y_pred_m)\n",
    "        return val_loss, acc_m, gap_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ec31cd-5bf6-488d-ac8e-007eb08f5694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_dim = 81313\n"
     ]
    }
   ],
   "source": [
    "# get dataframe\n",
    "df, out_dim = get_df()\n",
    "print(f\"out_dim = {out_dim}\")\n",
    "\n",
    "# get adaptive margin\n",
    "tmp = np.sqrt(1 / np.sqrt(df['landmark_id'].value_counts().sort_index().values))\n",
    "margins = (tmp - tmp.min()) / (tmp.max() - tmp.min()) * 0.45 + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b40235b-0dac-4ad5-9d62-6676aa691c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan/miniconda3/envs/FULL_ML/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:688: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get augmentations\n",
    "transforms_train, transforms_val = get_transforms()\n",
    "\n",
    "# get train and valid dataset\n",
    "df_train = df[df['fold'] != FOLD]\n",
    "df_valid = df[df['fold'] == FOLD].reset_index(drop=True).query(\"index % 15==0\")\n",
    "\n",
    "dataset_train = LandmarkDataset(df_train, 'train', transform=transforms_train)\n",
    "dataset_valid = LandmarkDataset(df_valid, 'val', transform=transforms_val)\n",
    "train_loader = DataLoader(\n",
    "    dataset_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42939722-e88c-439c-a9d9-6efc8bdcce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = nn.DataParallel(RexNet20_Landmark(out_dim=out_dim)).to(DEVICE)\n",
    "\n",
    "# loss func\n",
    "def criterion(logits_m, target):\n",
    "    arc = ArcFaceLossAdaptiveMargin(margins=margins, s=80)\n",
    "    loss_m = arc(logits_m, target, out_dim)\n",
    "    return loss_m\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scaler = GradScaler(enabled=True)\n",
    "\n",
    "# scheduler\n",
    "scheduler = OneCycleLR(optimizer, max_lr=LR, steps_per_epoch=len(train_loader), pct_start=.05, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "868a11b9-9a63-49f0-9d0f-b26bfb3a351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021Aug21_22H43M16S Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19756 [00:00<?, ?it/s]<ipython-input-5-b77f6e5d16d6>:14: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n",
      "  i = ctx.saved_variables[0]\n",
      "loss: 30.36711, smth: 33.90587:   1%|▏         | 288/19756 [01:47<2:01:34,  2.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-681bf4ee2241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgap_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-9e98c14bec33>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion, scaler)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mlogits_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-dc9cfe87846a>\u001b[0m in \u001b[0;36mcriterion\u001b[0;34m(logits_m, target)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0marc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArcFaceLossAdaptiveMargin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmargins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmargins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/FULL_ML/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/FULL_ML/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b77f6e5d16d6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, logits, labels, out_dim)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mcos_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0msin_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train & valid loop\n",
    "gap_m_max = 0.\n",
    "model_file = os.path.join(MODEL_DIR, f'{MODEL_NAME}_fold{FOLD}.pth')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    curr_time = datetime.strftime(datetime.now(), '%Y%b%d_%HH%MM%SS')\n",
    "    print(curr_time, 'Epoch:', epoch)\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler)\n",
    "    val_loss, acc_m, gap_m = val_epoch(model, valid_loader, criterion)\n",
    "\n",
    "    content = curr_time + ' ' + f'Fold {FOLD}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, valid loss: {(val_loss):.5f}, acc_m: {(acc_m):.6f}, gap_m: {(gap_m):.6f}.'\n",
    "    print(content)\n",
    "    \n",
    "    with open(os.path.join(MODEL_DIR, f'{MODEL_NAME}.txt'), 'a') as appender:\n",
    "        appender.write(content + '\\n')\n",
    "\n",
    "    print('gap_m_max ({:.6f} --> {:.6f}). Saving model ...'.format(gap_m_max, gap_m))\n",
    "    \n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        },\n",
    "        model_file\n",
    "    )\n",
    "    gap_m_max = gap_m\n",
    "\n",
    "print(datetime.strftime(datetime.now(), '%Y%b%d_%HH%MM%SS'), 'Training Finished!')\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, \n",
    "    os.path.join(MODEL_DIR, f'{MODEL_NAME}_fold{FOLD}_final.pth')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
