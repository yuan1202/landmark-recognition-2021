{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f0ca8ba-d913-45a6-a1ca-82a6bff8d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import Dict, Tuple, Any\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import cv2\n",
    "import albumentations\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09c899b-f994-41df-97be-54af973c7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../input/'\n",
    "LOAD_MODEL = 'effnetv2m_in21k_fold2_epoch8'\n",
    "\n",
    "IMAGE_SIZE = 600\n",
    "BATCH_SIZE = 38\n",
    "NUM_WORKERS = 4\n",
    "USE_AMP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf69705b-c586-4808-a152-f94863ca5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, csv, transform=None):\n",
    "\n",
    "        self.csv = csv.reset_index()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.csv.iloc[index]\n",
    "\n",
    "        image = cv2.imread(row.filepath)[:,:,::-1]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            res = self.transform(image=image)\n",
    "            image = res['image'].astype(np.float32)\n",
    "        else:\n",
    "            image = image.astype(np.float32)\n",
    "\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        \n",
    "        return torch.tensor(image)\n",
    "\n",
    "\n",
    "transforms = albumentations.Compose([\n",
    "    albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    albumentations.Normalize()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d592ec-3a42-49c0-acd4-7b0801807aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = torch.sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "\n",
    "class Swish_module(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return Swish.apply(x)\n",
    "\n",
    "\n",
    "class DenseCrossEntropy(nn.Module):\n",
    "    def forward(self, x, target):\n",
    "        x = x.float()\n",
    "        target = target.float()\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "\n",
    "        loss = -logprobs * target\n",
    "        loss = loss.sum(-1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class ArcMarginProduct_subcenter(nn.Module):\n",
    "    def __init__(self, in_features, out_features, k=3):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n",
    "        self.reset_parameters()\n",
    "        self.k = k\n",
    "        self.out_features = out_features\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n",
    "        cosine, _ = torch.max(cosine_all, dim=2)\n",
    "        return cosine   \n",
    "\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, p_trainable=True):\n",
    "        super(GeM,self).__init__()\n",
    "        if p_trainable:\n",
    "            self.p = Parameter(torch.ones(1)*p)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem(x, p=self.p, eps=self.eps)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n",
    "\n",
    "\n",
    "class EffnetV2m_Landmark(nn.Module):\n",
    "\n",
    "    def __init__(self, out_dim, load_pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = timm.create_model('tf_efficientnetv2_m_in21k', pretrained=False)\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.Linear(self.backbone.num_features, 512, bias=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            Swish_module()\n",
    "        )\n",
    "        self.backbone.global_pool = GeM()\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # self.swish = Swish_module()\n",
    "        self.metric_classify = ArcMarginProduct_subcenter(512, out_dim)\n",
    "\n",
    "\n",
    "    def extract(self, x):\n",
    "        return self.backbone(x)[:, :, 0, 0]\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.extract(x)\n",
    "        logits_m = self.metric_classify(self.feat(x))\n",
    "        return logits_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b38e724-5c19-45c8-ae5a-65875a9bf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = 81313\n",
    "\n",
    "load = torch.load('./model_checkpoints/{}.pth'.format(LOAD_MODEL))\n",
    "model_only_weight = {k[7:] if k.startswith('module.') else k: v for k, v in load['model_state_dict'].items()}\n",
    "\n",
    "model = EffnetV2m_Landmark(out_dim=out_dim).cuda()\n",
    "model.load_state_dict(model_only_weight)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76662800-0e82-413a-98d0-60d191c8474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe\n",
    "df = pd.read_csv('../input/recognition_solution_v2.1.csv')\n",
    "df['filepath'] = df['id'].apply(lambda x: os.path.join(DATA_DIR, 'test_2019', x[0], x[1], x[2], f'{x}.jpg'))\n",
    "\n",
    "dataset = LandmarkDataset(df, transform=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bfac030-c7f6-4a90-aee1-fda857b79fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3095/3095 [16:34<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# get top 10 similarity scores\n",
    "topK_cos_sims = np.zeros((len(df) , 10), dtype=np.float32)\n",
    "topK_indices = np.zeros((len(df) , 10), dtype=np.int64)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for idx, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        \n",
    "        data = data.cuda()\n",
    "\n",
    "        with autocast():\n",
    "            batch_cos_sims = model(data)\n",
    "            \n",
    "        batch_cos_sims, batch_indices = torch.topk(batch_cos_sims, k=10, dim=1)\n",
    "        \n",
    "        fill_idx0 = counter\n",
    "        fill_idx1 = counter + batch_cos_sims.shape[0]\n",
    "        \n",
    "        topK_cos_sims[fill_idx0:fill_idx1] = batch_cos_sims.cpu().numpy()\n",
    "        topK_indices[fill_idx0:fill_idx1] = batch_indices.cpu().numpy()\n",
    "        \n",
    "        counter += batch_cos_sims.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da426322-2ea6-442f-9f7a-9a6283ca0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./{}_test2019nonlandmark_LMvsnonLM_study_topK_cos_sims\".format(LOAD_MODEL), topK_cos_sims)\n",
    "np.save(\"./{}_test2019nonlandmark_LMvsnonLM_study_topK_indices\".format(LOAD_MODEL), topK_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2edccd2-f402-402e-9212-f85a72d156cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonLM_indices = df.loc[df['landmarks'].isna()].index.values\n",
    "LM_indices = df.loc[~df['landmarks'].isna()].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed21c70d-d5a2-4b98-81d7-be849f40d940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115605,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonLM_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43aef719-5b46-47d0-b9eb-f89b778ccc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "max1_cos_sims_nonLM = topK_cos_sims[nonLM_indices].max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f624a864-cebc-4628-8342-df79232d8e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24940668"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topK_cos_sims[nonLM_indices].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d13c12dd-734c-4c6a-8f2d-bb9ed979a448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66845703"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max1_cos_sims_nonLM.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51a70f75-222c-415c-b8dc-23de176563f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max1_cos_sims_LM = topK_cos_sims[LM_indices].max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90208c99-52fe-4132-b5ee-b401ca759e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74609375"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max1_cos_sims_LM.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "45309972-ac16-4429-bb4b-8176495303a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2717994"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topK_cos_sims[LM_indices].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d93f697-409b-483e-8b22-d654d530bc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115605,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max1_cos_sims_nonLM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0174780a-a534-4378-9f98-6913a524c828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1972,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max1_cos_sims_LM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aa7d44e5-65b0-48b4-a61d-e607671096b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39791,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max1_cos_sims_nonLM[max1_cos_sims_nonLM > .3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9f1c677-d0c4-4f91-96cc-613755cd6677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max1_cos_sims_LM[max1_cos_sims_LM < .3].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
